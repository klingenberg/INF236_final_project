\documentclass{article}

\usepackage[utf8]{inputenc}


\usepackage{geometry}
\usepackage{xcolor}
%\usepackage{graphix}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[backend=biber]{biblatex}


\addbibresource{citations.bib}


\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\definecolor{gradient1}{HTML}{833ab4}
\definecolor{gradient2}{HTML}{fd1d1d}
\definecolor{gradient3}{HTML}{fc8a3b}
\definecolor{gradient4}{HTML}{fcb045}


\geometry{top=2cm, bottom=2cm, left=2cm, right=2cm}

\title{Final Project\\ INF236: Parallel Programming\\ Parallel Matrix Multiplication}

\author{Kate\v{r}ina \v{C}\'{i}\v{z}kov\'{a}, Luca Klingenberg}


\begin{document}

\maketitle

\section{Introduction}
Our goal was to implement parallel matrix multiplication.
The standard algorithm has complexity $\mathcal{O}(n^3)$ and is straightforward
to parallelize. We used its implementation as a reference. We compared
it with Strassen's algorithm, which has complexity $\mathcal{O}(n^{\log{}7})$
but is harder to implement.

\section{Algorithms}
In this section all the implemented algorithms are explained in further detail.

\subsection{Matrix Multiplication}
The standard $\mathcal{O}(n^3)$ algorithm is simple. Each element $c_{ij}$ of 
the resulting matrix $\mathbf{C}$ is calculated as $c_{ij} =\sum_k a_{ik} \cdot b_{kj}$.
It is possible to implement the calculation with only three nested for-loops, but 
matrices are stored row-wise, so it is better to access it in consecutive order, in order to avoid cache misses.
Because of that, we used the following implementation. Complexity is still $\mathcal{O}(n^3)$,
but in practice it runs faster than with three nested loops that are indexed in $kij$ order.

\begin{algorithm}[H] 
\caption{Matrix Multiplication}
\label{alg:matmul}
\begin{algorithmic}[1]
\Require{$\mathbf{A}, \mathbf{B}$} %Input
\Ensure{$\mathbf{C}$ (the resulting matrix)} %Output
\Statex
\Function{matmul}{$\mathbf{A}, \mathbf{B}$}
	\For{$i=0, \ldots, n-1$}
		\For {$j=0, \ldots, n-1$}
			\State {$c[i][j] = 0$}
		\EndFor
		\For{$k=0, \ldots, n-1$}
			\For{$j=0, \ldots, n-1$}
				\State {$c[i][j] += a[i][k] \cdot b[k][j]$}
			\EndFor
		\EndFor
	\EndFor
	\State \Return {$\mathbf{C}$}
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Parallel Matrix Multiplication}
We just parallelized the outermost loop of the simple matrix multiplication algorithm
described in previous section.

\subsection{Strassen Algorithm}

The matrix multiplication can be formulated in terms of block matrices:
$$
\mathbf{A} \cdot \mathbf{B} =
\begin{pmatrix}
\mathbf{A}_{00} & \mathbf{A}_{01} \\
\mathbf{A}_{10} & \mathbf{A}_{11} 
\end{pmatrix}
\cdot
\begin{pmatrix}
\mathbf{B}_{00} & \mathbf{B}_{01} \\
\mathbf{B}_{10} & \mathbf{B}_{11} 
\end{pmatrix}
=
\begin{pmatrix}
\mathbf{A}_{00}\mathbf{B}_{00}+\mathbf{A}_{01}\mathbf{B}_{10} & \mathbf{A}_{00}\mathbf{B}_{01}+\mathbf{A}_{01}\mathbf{B}_{11} \\
\mathbf{A}_{10}\mathbf{B}_{00}+\mathbf{A}_{11}\mathbf{B}_{10} & \mathbf{A}_{10}\mathbf{B}_{01}+\mathbf{A}_{11}\mathbf{B}_{11} 
\end{pmatrix}
=
\begin{pmatrix}
\mathbf{C}_{00} & \mathbf{C}_{01} \\
\mathbf{C}_{10} & \mathbf{C}_{11} 
\end{pmatrix}
= \mathbf{C}
$$

According to this formula, it needs 8 multiplications of half-size matrices. 
The idea of Strassen's algorithm is to use more additions and subtractions,
but only 7 multiplications half-size matrices. The algorithm is used recursively
for these multiplications.

\begin{algorithm}[H] 
\caption{Strassen Matrix Multiplication}
\label{alg:strassen}
\begin{algorithmic}[1]
\Require{$\mathbf{A}, \mathbf{B}$} %Input
\Ensure{$\mathbf{C}$ (the resulting matrix)} %Output
\Statex
\Function{strassen}{$\mathbf{A}, \mathbf{B}, n$}
	\If {n == cutoff}
		\State \Return \Call{matmul}{$\mathbf{A}, \mathbf{B}$}
	\EndIf
	\State {$\mathbf{P}_1 = \Call{strassen}{\mathbf{A}_{00} + \mathbf{A}_{11},  \mathbf{B}_{00} + \mathbf{B}_{11}, \frac{n}{2}$}}
	\State {$\mathbf{P}_2 = \Call{strassen}{\mathbf{A}_{10} + \mathbf{A}_{11},  \mathbf{B}_{00}, \frac{n}{2}$}}
	\State {$\mathbf{P}_3 = \Call{strassen}{\mathbf{A}_{00},  \mathbf{B}_{01} - \mathbf{B}_{11}, \frac{n}{2}$}}
	\State {$\mathbf{P}_4 = \Call{strassen}{\mathbf{A}_{11},  \mathbf{B}_{10} - \mathbf{B}_{00}, \frac{n}{2}$}}
	\State {$\mathbf{P}_5 = \Call{strassen}{\mathbf{A}_{00} + \mathbf{A}_{01},  \mathbf{B}_{11}, \frac{n}{2}$}}
	\State {$\mathbf{P}_6 = \Call{strassen}{\mathbf{A}_{10} - \mathbf{A}_{00},  \mathbf{B}_{00} + \mathbf{B}_{01}, \frac{n}{2}$}}
	\State {$\mathbf{P}_7 = \Call{strassen}{\mathbf{A}_{01} - \mathbf{A}_{11},  \mathbf{B}_{10} + \mathbf{B}_{11}, \frac{n}{2}$}}
	\State {$\mathbf{C}_{00} = \mathbf{P}_1 + \mathbf{P}_4 - \mathbf{P}_5 + \mathbf{P}_7$}
	\State {$\mathbf{C}_{01} = \mathbf{P}_3 + \mathbf{P}_5$}
	\State {$\mathbf{C}_{10} = \mathbf{P}_2 + \mathbf{P}_4$}
	\State {$\mathbf{C}_{11} = \mathbf{P}_1 - \mathbf{P}_2 + \mathbf{P}_3 + \mathbf{P}_6$}
	 	
	\State \Return {$\mathbf{C}$}
\EndFunction
\end{algorithmic}
\end{algorithm}

We used Winograd's version of Strassen's algorithm described in \cite{boyer2009memory}.
It needs only 15 additions and subtractions instead of 18 because some results of 
these additions and subtractions are reused. We also used scheduling table 1, 
therefore our implementation needs only two temporary matrices in each recursive call
to store intermediate results.

Strassen's algorithm is recursive and it is working with quarters of input matrices.
Because of that, it is useful to use z-ordering for storing matrices in our implementation.
Its advantage is, that all submatrices are stored in consecutive parts of memory.
But we switch from Strassen's algorithm to the simple matrix multiplication
when the matrices are too small so Strassen's algorithm overhead is too big.
For the simple matrix multiplication is better to have the matrices ordered row-wise.
We combined both and used the z-ordering down to the cutting point of recursion.
We left the small submatrices which are multiplied with the simple multiplication algorithm
in row-wise order. %The reordering functions are implemented in TODO file.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=.3]{z_ordering.pdf}}
\caption{z-ordering diagram}
\label{fig}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[scale=.4]{partly_z_ordering.pdf}}
\caption{z-ordering only for bigger submatrices}
\label{fig}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[scale=.3]{mem_ordering.pdf}}
\caption{z-ordered matrix stored in memory}
\label{fig}
\end{figure}

\subsection{Parallel Strassen Algorithm}

It is straightforward to parallelize additions and subtractions in Strassen's algorithm.
The problem is that for small matrices it is not worth it to initialize a parallel region.
Because of that, we can not just insert \texttt{\#pragma omp parallel for} before
the addition and subtraction for-loop. 

Then, we were thinking about parallelizing recursive calls -- calculate $\mathbf{P}_1$ to $\mathbf{P}_7$ in parallel.
But since we are using only two temporary matrices in each recursive call, it is necessary to finish
a recursive call before starting the next one. This may be solved using more space for intermediate
results, but we did not try that.

\section{Experiments}
	
\begin{tikzpicture}
\begin{axis}[
    title={Sequential Strassen Algorithm with different values for the cutoff level},
    xlabel={Cuttoff},
    ylabel={Speedup},
    xmin=1, xmax=512,
    ymin=0, ymax=4,
    xtick={0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512},
    ytick={0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4},
	xmode=log,
    log basis x={2},
    legend pos=outer north east,
    xmajorgrids=true,
    ymajorgrids=true,
    grid style=dashed,
]

	% running time of normal sequential: 22.729855
	\addplot[color=gradient1, mark=*, only marks] table [x=cutoff, y=size1, col sep=comma] {cutoffs.csv};
	% running time of normal sequential: 1.397876
	\addplot[color=gradient2, mark=*, only marks] table [x=cutoff, y=size2, col sep=comma] {cutoffs.csv};
	% running time of normal sequential: 0.177942
	\addplot[color=gradient3, mark=*, only marks] table [x=cutoff, y=size3, col sep=comma] {cutoffs.csv};
	% running time of normal sequential: 0.032513
	\addplot[color=gradient4, mark=*, only marks] table [x=cutoff, y=size4, col sep=comma] {cutoffs.csv};
    
    \legend{2048 $\times$ 2048, 1024 $\times$ 1024, 512 $\times$ 512, 256 $\times$ 256}
\end{axis}
\end{tikzpicture}

\begin{tikzpicture}
\begin{axis}[
    title={Sequential Strassen Algorithm with different values for the cutoff level},
    xlabel={Cuttoff},
    ylabel={Speedup},
    xmin=1, xmax=512,
    ymin=0, ymax=4,
    xtick={0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512},
    ytick={0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4},
	xmode=log,
    log basis x={2},
    legend pos=outer north east,
    xmajorgrids=true,
    ymajorgrids=true,
    grid style=dashed,
]

	% running time of normal sequential: 22.729855
	\addplot[color=gradient1, mark=*, only marks] table [x=cutoff, y=size1, col sep=comma] {cutoffs.csv};
	% running time of normal sequential: 1.397876
	\addplot[color=gradient2, mark=*, only marks] table [x=cutoff, y=size2, col sep=comma] {cutoffs.csv};
	% running time of normal sequential: 0.177942
	\addplot[color=gradient3, mark=*, only marks] table [x=cutoff, y=size3, col sep=comma] {cutoffs.csv};
	% running time of normal sequential: 0.032513
	\addplot[color=gradient4, mark=*, only marks] table [x=cutoff, y=size4, col sep=comma] {cutoffs.csv};
    
    \legend{2048 $\times$ 2048, 1024 $\times$ 1024, 512 $\times$ 512, 256 $\times$ 256}
\end{axis}
\end{tikzpicture}


\section{Conclusion}

\printbibliography

\end{document}